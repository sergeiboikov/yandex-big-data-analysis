{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hive Assignment 1. DDL: Create Tables\n",
    "\n",
    "The purpose of this task is to create an external table on the posts data of the stackoverflow.com website.\n",
    "\n",
    "Create your own database and 'use' it. \n",
    "Create external table 'posts_sample_external' over the sample dataset with posts in '/data/stackexchange1000' directory. \n",
    "Create managed table 'posts_sample' and populate with the data from the external table. \n",
    "'Posts_sample' table should be partitioned by year and by month of post creation. \n",
    "Provide output of query which selects lines number per each partition in the format:\n",
    "\n",
    "`year <tab> month <tab> lines count`\n",
    "    \n",
    "where year in YYYY format and month in YYYY-MM format. The result is the 3th line of the last query output.\n",
    "\n",
    "The result on the sample dataset:\n",
    "\n",
    "`2008    2008-10 73`\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Create DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing creation_db.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a creation_db.hql\n",
    "CREATE DATABASE demodb LOCATION '/user/jovyan/somemetastore';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-1.1.0-bin/lib/hive-common-1.1.0.jar!/hive-log4j.properties\n",
      "OK\n",
      "Time taken: 0.555 seconds\n"
     ]
    }
   ],
   "source": [
    "#! hive -f creation_db.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Create External Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing external_table.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile external_table.hql\n",
    "\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "\n",
    "USE demodb;\n",
    "DROP TABLE IF EXISTS posts_sample_external;\n",
    "\n",
    "CREATE EXTERNAL TABLE posts_sample_external (\n",
    "    Id STRING,\n",
    "    PostTypeId STRING,\n",
    "    CreationDate STRING,\n",
    "    Tags STRING,\n",
    "    OwnerUserId STRING,\n",
    "    ParentId STRING,\n",
    "    Score STRING,\n",
    "    FavoriteCount STRING\n",
    ")\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'\n",
    "WITH SERDEPROPERTIES (\n",
    "    \"input.regex\" = '^(?=.*\\\\brow Id=\"(\\\\d+))(?=.*\\\\bPostTypeId=\"(1|2))(?=.*\\\\bCreationDate=\"(\\\\d{4}-\\\\d{2}-\\\\d{2}T\\\\d{2}:\\\\d{2}:\\\\d{2}.\\\\d+))(?=.*\\\\bTags=\"([^\\\\\"]+))?(?=.*\\\\bOwnerUserId=\"(\\\\d+))?(?=.*\\\\bParentId=\"(\\\\d+))?(?=.*\\\\bScore=\"(\\\\d+))?(?=.*\\\\bFavoriteCount=\"(\\\\d+))?.*$'\n",
    ")\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/data/stackexchange1000/posts';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hive -S -f external_table.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Create Managed Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing managed_table.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile managed_table.hql\n",
    "\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "\n",
    "USE demodb;\n",
    "DROP TABLE IF EXISTS posts_sample;\n",
    "\n",
    "CREATE TABLE posts_sample (\n",
    "    Id INT,\n",
    "    PostTypeId INT,\n",
    "    CreationDate STRING,\n",
    "    Tags STRING,\n",
    "    OwnerUserId INT,\n",
    "    ParentId INT,\n",
    "    Score INT,\n",
    "    FavoriteCount INT\n",
    ")\n",
    "PARTITIONED BY(year STRING, month STRING);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hive -S -f managed_table.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Populate the managed table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing populate_managed_table.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile populate_managed_table.hql\n",
    "\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "\n",
    "set hive.exec.dynamic.partition.mode=nonstrict\n",
    "set hive.error.on.empty.partition=true\n",
    "\n",
    "USE demodb;\n",
    "FROM demodb.posts_sample_external\n",
    "INSERT OVERWRITE TABLE demodb.posts_sample\n",
    "PARTITION (year, month)\n",
    "SELECT Id,\n",
    "       PostTypeId,\n",
    "       CreationDate,\n",
    "       Tags,\n",
    "       OwnerUserId,\n",
    "       ParentId,\n",
    "       Score,\n",
    "       FavoriteCount,\n",
    "       year(CreationDate) as year,\n",
    "       month(CreationDate) as month\n",
    "WHERE CreationDate IS NOT NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Value had a \\n character in it.\r\n"
     ]
    }
   ],
   "source": [
    "! hive -S -f populate_managed_table.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing query.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile query.hql\n",
    "USE demodb;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to query.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a query.hql\n",
    "SELECT t.year, t.month_c as month, t.lines\n",
    "FROM (\n",
    "    SELECT YEAR(CreationDate) as year\n",
    "        , MONTH(CreationDate) as month\n",
    "        , CONCAT(YEAR(CreationDate),'-',MONTH(CreationDate)) as month_c\n",
    "        , COUNT(1) as lines \n",
    "        , ROW_NUMBER() over (ORDER BY YEAR(CreationDate), MONTH(CreationDate)) as RowNum\n",
    "    FROM posts_sample\n",
    "    GROUP BY YEAR(CreationDate), MONTH(CreationDate), CONCAT(YEAR(CreationDate),'-',MONTH(CreationDate))\n",
    "    ORDER BY year, month\n",
    ") t\n",
    "WHERE t.RowNum = 3;\n",
    "#HINT for submission\n",
    "#SELECT 2008 as year, \"2008-10\" as month, 73 as lines;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-1.1.0-bin/lib/hive-common-1.1.0.jar!/hive-log4j.properties\n",
      "OK\n",
      "Time taken: 0.441 seconds\n",
      "Query ID = jovyan_20200910060404_2b415540-95c8-434d-b20d-9a378491daf5\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1599716343617_0002, Tracking URL = http://33f85178d43a:8088/proxy/application_1599716343617_0002/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1599716343617_0002\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2020-09-10 06:04:08,012 Stage-1 map = 0%,  reduce = 0%\n",
      "2020-09-10 06:04:13,276 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.64 sec\n",
      "2020-09-10 06:04:18,501 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.68 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 680 msec\n",
      "Ended Job = job_1599716343617_0002\n",
      "Launching Job 2 out of 3\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1599716343617_0003, Tracking URL = http://33f85178d43a:8088/proxy/application_1599716343617_0003/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1599716343617_0003\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2020-09-10 06:04:30,670 Stage-2 map = 0%,  reduce = 0%\n",
      "2020-09-10 06:04:34,797 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 0.68 sec\n",
      "2020-09-10 06:04:40,975 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.14 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 140 msec\n",
      "Ended Job = job_1599716343617_0003\n",
      "Launching Job 3 out of 3\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1599716343617_0004, Tracking URL = http://33f85178d43a:8088/proxy/application_1599716343617_0004/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1599716343617_0004\n",
      "Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1\n",
      "2020-09-10 06:04:52,581 Stage-3 map = 0%,  reduce = 0%\n",
      "2020-09-10 06:04:55,775 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 0.63 sec\n",
      "2020-09-10 06:05:01,929 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.49 sec\n",
      "MapReduce Total cumulative CPU time: 1 seconds 490 msec\n",
      "Ended Job = job_1599716343617_0004\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.68 sec   HDFS Read: 2934306 HDFS Write: 3233 SUCCESS\n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 2.14 sec   HDFS Read: 10066 HDFS Write: 126 SUCCESS\n",
      "Stage-Stage-3: Map: 1  Reduce: 1   Cumulative CPU: 1.49 sec   HDFS Read: 4862 HDFS Write: 16 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 7 seconds 310 msec\n",
      "OK\n",
      "2008\t2008-10\t73\n",
      "Time taken: 62.11 seconds, Fetched: 1 row(s)\n"
     ]
    }
   ],
   "source": [
    "! hive -f query.hql"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
