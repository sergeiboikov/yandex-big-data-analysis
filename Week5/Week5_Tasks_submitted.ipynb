{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph based Music Recommender\n",
    "\n",
    "In this assignments you will build a music recommender engine based on user’s playlists history. For Tasks 1-4 you will use dataframes that represent the weight of edges count the collaborative similarity between the vertices. Task 5-6 takes steps forward to fully implement the recommender system described in the lectures.\n",
    "\n",
    "## Data description\n",
    "There are two data sources for this assignment. They are DataFrames in parquet format.\n",
    "\n",
    "**The first dataset captures the user’s playing history.**\n",
    "\n",
    "*Location - /data/sample264*\n",
    "\n",
    "Fields: *trackId, userId, timestamp, artistId*\n",
    "\n",
    "- trackId - id of the track\n",
    "- userId - id of the user\n",
    "- artistId - id of the artist\n",
    "- timestamp - timestamp of the moment the user starts listening to a track\n",
    "\n",
    "**The second is the meta data for track or artist.**\n",
    "\n",
    "*Location - /data/meta*\n",
    "\n",
    "Fields: *type, Name, Artist, Id*\n",
    "\n",
    "- Type could be “track” or “artist”\n",
    "- Name is the title of the track, if the type == “track” and the name of the musician or group, if the type == “artist”.\n",
    "- Artist states for the creator of the track in case the type == “track” and for the name of the musician or group in case the type == “artist”.\n",
    "- Id - id of the item\n",
    "\n",
    "**NB.** Each subsequent of these tasks is a continuation of the previous one. So, you may use the same ipython notebook for all the programming assignments in this week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph based Music Recommender. Task 1\n",
    "Build the edges of the type “track-track”. To do it you will need to count the collaborative similarity between all the tracks: if a user has started listening to track B within 7 minutes after starting track A, then you should add 1 to the weight of the edge from vertex A to vertex B (initial weight is equal to 0).\n",
    "\n",
    "Example:\n",
    "\n",
    "`userId artistId trackId timestamp\n",
    "7        12        1          1534574189\n",
    "7        13        4          1534574289 \n",
    "5        12        1          1534574389 \n",
    "5        13        4          1534594189 \n",
    "6        12        1          1534574489 \n",
    "6        13        4          1534574689` \n",
    "\n",
    "The track 1 is similar to the track 4 with the weight 2 (before normalization): the user 7 and the user 6 listened these 2 tracks together in the 7 minutes long window:\n",
    "\n",
    "- userId 7: 1534574289 - 1534574189 = 100 seconds = 1 min 40 seconds < 7 minutes\n",
    "- userId 6: 1534574689 - 1534574489 = 200 seconds = 3 min 20 seconds < 7 minutes\n",
    "Note that the track 4 is similar to the track 1 with the same weight 2.\n",
    "\n",
    "**Tip:** consider joining the graph to itself with the UserId and remove pairs with the same tracks.For each track choose top 50 tracks ordered by weight similar to it and normalize weights of its edges (divide the weight of each edge on a sum of weights of all edges). Use rank() to choose top 40 tracks as is done in the demo.\n",
    "\n",
    "Sort the resulting Data Frame in the descending order by the column norm_weight, and then in the ascending order this time first by “id1”, then by “id2”. Take top 40 rows, select only the columns “id1”, “id2”, and print the columns “id1”, “id2” of the resulting dataframe.\n",
    "\n",
    "**Output example:**\n",
    "\n",
    "`54719\t\t767867\n",
    "54719\t\t767866\n",
    "50787\t\t327676`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.1.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.12 (default, Nov 19 2016 06:48:10)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "#execfile(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sparkSession = SparkSession.builder.enableHiveSupport().master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sparkSession.read.parquet(\"/data/sample264\")\n",
    "meta = sparkSession.read.parquet(\"/data/meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, sum\n",
    "\n",
    "def norm(df, key1, key2, field, n): \n",
    "    \n",
    "    window = Window.partitionBy(key1).orderBy(col(field).desc())\n",
    "    \n",
    "    topsDF = df.withColumn(\"row_number\", row_number().over(window)) \\\n",
    "        .filter(col(\"row_number\") <= n) \\\n",
    "        .drop(col(\"row_number\")) \n",
    "\n",
    "    tmpDF = topsDF.groupBy(col(key1)).agg(col(key1), sum(col(field)).alias(\"sum_\" + field))\n",
    "\n",
    "    normalizedDF = topsDF.join(tmpDF, key1, \"inner\") \\\n",
    "        .withColumn(\"norm_\" + field, col(field) / col(\"sum_\" + field)) \\\n",
    "        .cache()\n",
    "\n",
    "    return normalizedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list, col, abs, count, rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.select(col(\"userId\"), col(\"trackId\").alias(\"trackId1\"), col(\"timestamp\").alias(\"timestamp1\"));\n",
    "    \n",
    "data2 = data.select(col(\"userId\"), col(\"trackId\").alias(\"trackId2\"), col(\"timestamp\").alias(\"timestamp2\"));\n",
    "\n",
    "#joining the graph to itself with the UserId\n",
    "#...and remove pairs with the same tracks.\n",
    "similarityDF = data1.join(data2, \"userId\", \"inner\")\\\n",
    "    .filter(abs(col(\"timestamp1\")-col(\"timestamp2\"))/60 <= 7)\\\n",
    "    .filter(col(\"trackId1\") != col(\"trackId2\"))\\\n",
    "    .groupBy(col('trackId1'), col('trackId2'))\\\n",
    "    .count().alias('count')\\\n",
    "    .cache();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose top 50 tracks ordered by weight similar to it and normalize weights of its edges\n",
    "normalizedDF = norm(similarityDF, \"trackId1\", \"trackId2\", \"count\", 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.orderBy(col(\"norm_count\").desc())\n",
    "\n",
    "#Sort the resulting Data Frame in the descending order by the column norm_weight, \n",
    "#and then in the ascending order this time first by “id1”, then by “id2”. \n",
    "#Take top 40 rows, select only the columns “id1”, “id2”, and print the columns “id1”, “id2” of the resulting dataframe.    \n",
    "similarTrackList = normalizedDF.withColumn(\"position\", rank().over(window)) \\\n",
    "    .orderBy(col(\"norm_count\"), col(\"trackId1\"), col(\"trackId2\"))\\\n",
    "    .select(col(\"trackId1\"), col(\"trackId2\"))\\\n",
    "    .filter(col(\"position\") < 50)\\\n",
    "    .take(40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for val in result:\n",
    "#    print('%s %s' % val);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph based Music Recommender. Task 2\n",
    "\n",
    "Build the edges of the type “user-track”. Take the amount of times the track was listened by the user as the weight of the edge from the user’s vertex to the track’s vertex.\n",
    "\n",
    "**Tip:** group the dataframe by columns userId and trackId and use function “count” of DF API.\n",
    "\n",
    "For each user take top-1000 and normalize them.\n",
    "\n",
    "Sort the resulting Data Frame in descending order by the column norm_weight, and then in ascending order this time first by “id1”, then by “id2”. Take top 40 rows, select only the columns “id1”, “id2”, and print the columns “id1”, “id2” of the resulting dataframe.\n",
    "\n",
    "**The part of the result on the sample dataset:**\n",
    "\n",
    "`...\n",
    "195 946408\n",
    "215 860111\n",
    "235 897176\n",
    "300 857973\n",
    "321 915545\n",
    "...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group the dataframe by columns userId and trackId and use function “count” of DF API.\n",
    "userTrack = data.groupBy(col(\"userId\"), col(\"trackId\")).count();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each user take top-1000 and normalize them.\n",
    "#Sort the resulting Data Frame in descending order by the column norm_weight, \n",
    "#and then in ascending order this time first by “id1”, then by “id2”.\n",
    "userTrackNorm = (norm(userTrack, \"userId\", \"trackId\", \"count\", 1000) \\\n",
    "    .orderBy(col(\"norm_count\").desc(), col(\"userId\"), col(\"trackId\"))\\\n",
    "    .limit(40)).cache();    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.orderBy(col(\"norm_count\"));\n",
    "\n",
    "#Take top 40 rows, select only the columns “id1”, “id2”, and print the columns “id1”, “id2” of the resulting dataframe.\n",
    "userTrackList = userTrackNorm.withColumn(\"position\", rank().over(window))\\\n",
    "    .filter(col(\"position\") < 50)\\\n",
    "    .select(col(\"userId\").alias(\"id1\"), col(\"trackId\").alias(\"id2\"))\\\n",
    "    .take(40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 965774\n",
      "116 867268\n",
      "128 852564\n",
      "131 880170\n",
      "195 946408\n",
      "215 860111\n",
      "235 897176\n",
      "300 857973\n",
      "321 915545\n",
      "328 943482\n",
      "333 818202\n",
      "346 864911\n",
      "356 961308\n",
      "428 943572\n",
      "431 902497\n",
      "445 831381\n",
      "488 841340\n",
      "542 815388\n",
      "617 946395\n",
      "649 901672\n",
      "658 937522\n",
      "662 881433\n",
      "698 935934\n",
      "708 952432\n",
      "746 879259\n",
      "747 879259\n",
      "776 946408\n",
      "784 806468\n",
      "806 866581\n",
      "811 948017\n",
      "837 799685\n",
      "901 871513\n",
      "923 879322\n",
      "934 940714\n",
      "957 945183\n",
      "989 878364\n",
      "999 967768\n",
      "1006 962774\n",
      "1049 849484\n",
      "1057 920458\n"
     ]
    }
   ],
   "source": [
    "#for val in userTrackList:\n",
    "#    print (\"%s %s\" % val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph based Music Recommender. Task 3\n",
    "Build the edges of the type “user-artist”. Take the amount of times the user has listened to the artist’s tracks as the weight of the edge from the user’s vertex to the artist’s vertex.\n",
    "\n",
    "**Tip:** group the dataframe by the columns userId and trackId and use the function “count” of DF API. For each user take top-100 artists and normalize weights.\n",
    "\n",
    "Sort the resulting Data Frame in descending order by the column norm_weight, and then in ascending order this time first by “id1”, then by “id2”. Take top 40 rows, select only the columns “id1”, “id2”, and print the columns “id1”, “id2” of the resulting dataframe.\n",
    "\n",
    "**The part of the result on the sample dataset:**\n",
    "\n",
    "`...\n",
    "131 983068\n",
    "195 997265\n",
    "215 991696\n",
    "235 990642\n",
    "288 1000564\n",
    "...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "userArtist = data.groupBy(col(\"userId\"), col(\"artistId\")).count();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "userArtistNorm = (norm(userArtist, \"userId\", \"artistId\", \"count\", 100)\\\n",
    "                  .orderBy(col(\"norm_count\").desc(), col(\"userId\"), col(\"artistId\"))\\\n",
    "                  .limit(40)).cache();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.orderBy(col(\"norm_count\"));\n",
    "\n",
    "userArtistList = userArtistNorm.withColumn(\"position\", rank().over(window))\\\n",
    "    .filter(col(\"position\") < 40)\\\n",
    "    .select(col(\"userId\").alias(\"id1\"), col(\"artistId\").alias(\"id2\"))\\\n",
    "    .take(40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 993426\n",
      "116 974937\n",
      "128 1003021\n",
      "131 983068\n",
      "195 997265\n",
      "215 991696\n",
      "235 990642\n",
      "288 1000564\n",
      "300 1003362\n",
      "321 986172\n",
      "328 967986\n",
      "333 1000416\n",
      "346 982037\n",
      "356 974846\n",
      "374 1003167\n",
      "428 993161\n",
      "431 969340\n",
      "445 970387\n",
      "488 970525\n",
      "542 969751\n",
      "612 987351\n",
      "617 970240\n",
      "649 973851\n",
      "658 973232\n",
      "662 975279\n",
      "698 995788\n",
      "708 968848\n",
      "746 972032\n",
      "747 972032\n",
      "776 997265\n",
      "784 969853\n",
      "806 995126\n",
      "811 996436\n",
      "837 989262\n",
      "901 988199\n",
      "923 977066\n",
      "934 990860\n",
      "957 991171\n",
      "989 975339\n",
      "999 968823\n"
     ]
    }
   ],
   "source": [
    "#for val in userArtistList:\n",
    "#    print (\"%s %s\" % val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph based Music Recommender. Task 4\n",
    "\n",
    "Build the edges of the type “artist-track”. Take the amount of times the track HAS BEEN listened by all users as the weight of the edge from the artist’s vertex to the track’s vertex.\n",
    "\n",
    "**Tip:** group the dataframe by the columns “artistId” and “trackId” and use the function “count” of DF API. For each artist take top-100 tracks and normalize weights.\n",
    "\n",
    "Sort the resulting Data Frame in descending order by the column norm_weight, and then in ascending order this time first by “id1”, then by “id2”. Take top 40 rows, select only the columns “id1”, “id2”, and print the columns “id1”, “id2” of the resulting dataframe.\n",
    "\n",
    "**The part of the result on the sample dataset:**\n",
    "\n",
    "`...\n",
    "968017 859321\n",
    "968022 852786\n",
    "968034 807671\n",
    "968038 964150\n",
    "968042 835935\n",
    "...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "artistTrack = data.groupBy(col(\"artistId\"), col(\"trackId\")).count();\n",
    "\n",
    "artistTrackNorm = (norm(artistTrack, \"artistId\", \"trackId\", \"count\", 100)\\\n",
    "                   .orderBy(col(\"norm_count\").desc(), col(\"artistId\"), col(\"trackId\"))\\\n",
    "                   .limit(40)).cache();\n",
    "\n",
    "artistTrackList = artistTrackNorm.select(col(\"artistId\").alias(\"id1\"), col(\"trackId\").alias(\"id2\")).take(40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "967993 869415\n",
      "967998 947428\n",
      "968004 927380\n",
      "968017 859321\n",
      "968022 852786\n",
      "968034 807671\n",
      "968038 964150\n",
      "968042 835935\n",
      "968043 913568\n",
      "968046 935077\n",
      "968047 806127\n",
      "968065 907906\n",
      "968073 964586\n",
      "968086 813446\n",
      "968092 837129\n",
      "968118 914441\n",
      "968125 821410\n",
      "968140 953008\n",
      "968148 877445\n",
      "968161 809793\n",
      "968163 803065\n",
      "968168 876119\n",
      "968189 858639\n",
      "968221 896937\n",
      "968224 892880\n",
      "968232 825536\n",
      "968237 932845\n",
      "968238 939177\n",
      "968241 879045\n",
      "968242 911250\n",
      "968248 953554\n",
      "968255 808494\n",
      "968259 880230\n",
      "968265 950148\n",
      "968266 824437\n",
      "968269 913243\n",
      "968272 816049\n",
      "968278 946743\n",
      "968285 847460\n",
      "968286 940006\n"
     ]
    }
   ],
   "source": [
    "for val in artistTrackList:\n",
    "    print (\"%s %s\" % val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
