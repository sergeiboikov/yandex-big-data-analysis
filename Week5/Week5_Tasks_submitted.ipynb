{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph based Music Recommender\n",
    "\n",
    "In this assignments you will build a music recommender engine based on user’s playlists history. For Tasks 1-4 you will use dataframes that represent the weight of edges count the collaborative similarity between the vertices. Task 5-6 takes steps forward to fully implement the recommender system described in the lectures.\n",
    "\n",
    "## Data description\n",
    "There are two data sources for this assignment. They are DataFrames in parquet format.\n",
    "\n",
    "**The first dataset captures the user’s playing history.**\n",
    "\n",
    "*Location - /data/sample264*\n",
    "\n",
    "Fields: *trackId, userId, timestamp, artistId*\n",
    "\n",
    "- trackId - id of the track\n",
    "- userId - id of the user\n",
    "- artistId - id of the artist\n",
    "- timestamp - timestamp of the moment the user starts listening to a track\n",
    "\n",
    "**The second is the meta data for track or artist.**\n",
    "\n",
    "*Location - /data/meta*\n",
    "\n",
    "Fields: *type, Name, Artist, Id*\n",
    "\n",
    "- Type could be “track” or “artist”\n",
    "- Name is the title of the track, if the type == “track” and the name of the musician or group, if the type == “artist”.\n",
    "- Artist states for the creator of the track in case the type == “track” and for the name of the musician or group in case the type == “artist”.\n",
    "- Id - id of the item\n",
    "\n",
    "**NB.** Each subsequent of these tasks is a continuation of the previous one. So, you may use the same ipython notebook for all the programming assignments in this week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph based Music Recommender. Task 1\n",
    "Build the edges of the type “track-track”. To do it you will need to count the collaborative similarity between all the tracks: if a user has started listening to track B within 7 minutes after starting track A, then you should add 1 to the weight of the edge from vertex A to vertex B (initial weight is equal to 0).\n",
    "\n",
    "Example:\n",
    "\n",
    "`userId artistId trackId timestamp\n",
    "7        12        1          1534574189\n",
    "7        13        4          1534574289 \n",
    "5        12        1          1534574389 \n",
    "5        13        4          1534594189 \n",
    "6        12        1          1534574489 \n",
    "6        13        4          1534574689` \n",
    "\n",
    "The track 1 is similar to the track 4 with the weight 2 (before normalization): the user 7 and the user 6 listened these 2 tracks together in the 7 minutes long window:\n",
    "\n",
    "- userId 7: 1534574289 - 1534574189 = 100 seconds = 1 min 40 seconds < 7 minutes\n",
    "- userId 6: 1534574689 - 1534574489 = 200 seconds = 3 min 20 seconds < 7 minutes\n",
    "Note that the track 4 is similar to the track 1 with the same weight 2.\n",
    "\n",
    "**Tip:** consider joining the graph to itself with the UserId and remove pairs with the same tracks.For each track choose top 50 tracks ordered by weight similar to it and normalize weights of its edges (divide the weight of each edge on a sum of weights of all edges). Use rank() to choose top 40 tracks as is done in the demo.\n",
    "\n",
    "Sort the resulting Data Frame in the descending order by the column norm_weight, and then in the ascending order this time first by “id1”, then by “id2”. Take top 40 rows, select only the columns “id1”, “id2”, and print the columns “id1”, “id2” of the resulting dataframe.\n",
    "\n",
    "**Output example:**\n",
    "\n",
    "`54719\t\t767867\n",
    "54719\t\t767866\n",
    "50787\t\t327676`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#execfile(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sparkSession = SparkSession.builder.enableHiveSupport().master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sparkSession.read.parquet(\"/data/sample264\")\n",
    "meta = sparkSession.read.parquet(\"/data/meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, sum\n",
    "\n",
    "def norm(df, key1, key2, field, n): \n",
    "    \n",
    "    window = Window.partitionBy(key1).orderBy(col(field).desc())\n",
    "    \n",
    "    topsDF = df.withColumn(\"row_number\", row_number().over(window)) \\\n",
    "        .filter(col(\"row_number\") <= n) \\\n",
    "        .drop(col(\"row_number\")) \n",
    "\n",
    "    tmpDF = topsDF.groupBy(col(key1)).agg(col(key1), sum(col(field)).alias(\"sum_\" + field))\n",
    "\n",
    "    normalizedDF = topsDF.join(tmpDF, key1, \"inner\") \\\n",
    "        .withColumn(\"norm_\" + field, col(field) / col(\"sum_\" + field)) \\\n",
    "        .cache()\n",
    "\n",
    "    return normalizedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list, col, abs, count, rank\n",
    "\n",
    "data1 = data.select(col(\"userId\"), col(\"trackId\").alias(\"trackId1\"), col(\"timestamp\").alias(\"timestamp1\"));\n",
    "    \n",
    "data2 = data.select(col(\"userId\"), col(\"trackId\").alias(\"trackId2\"), col(\"timestamp\").alias(\"timestamp2\"));\n",
    "\n",
    "#joining the graph to itself with the UserId\n",
    "#...and remove pairs with the same tracks.\n",
    "similarityDF = data1.join(data2, \"userId\", \"inner\")\\\n",
    "    .filter(abs(col(\"timestamp1\")-col(\"timestamp2\"))/60 <= 7)\\\n",
    "    .filter(col(\"trackId1\") != col(\"trackId2\"))\\\n",
    "    .groupBy(col('trackId1'), col('trackId2'))\\\n",
    "    .count().alias('count')\\\n",
    "    .cache();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose top 50 tracks ordered by weight similar to it and normalize weights of its edges\n",
    "normalizedDF = norm(similarityDF, \"trackId1\", \"trackId2\", \"count\", 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.orderBy(col(\"norm_count\").desc())\n",
    "\n",
    "similarTrackList = normalizedDF.withColumn(\"position\", rank().over(window)) \\\n",
    "    .orderBy(col(\"norm_count\"), col(\"trackId1\"), col(\"trackId2\"))\\\n",
    "    .filter(col(\"position\") < 50);\n",
    "\n",
    "#Sort the resulting Data Frame in the descending order by the column norm_weight, \n",
    "#and then in the ascending order this time first by “id1”, then by “id2”. \n",
    "#Take top 40 rows, select only the columns “id1”, “id2”, and print the columns “id1”, “id2” of the resulting dataframe.    \n",
    "result = similarTrackList\\\n",
    "    .select(col(\"trackId1\"), col(\"trackId2\"))\\\n",
    "    .orderBy(col(\"trackId1\"), col(\"trackId2\"))\\\n",
    "    .take(40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for val in result:\n",
    "#    print('%s %s' % val);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph based Music Recommender. Task 2\n",
    "\n",
    "Build the edges of the type “user-track”. Take the amount of times the track was listened by the user as the weight of the edge from the user’s vertex to the track’s vertex.\n",
    "\n",
    "**Tip:** group the dataframe by columns userId and trackId and use function “count” of DF API.\n",
    "\n",
    "For each user take top-1000 and normalize them.\n",
    "\n",
    "Sort the resulting Data Frame in descending order by the column norm_weight, and then in ascending order this time first by “id1”, then by “id2”. Take top 40 rows, select only the columns “id1”, “id2”, and print the columns “id1”, “id2” of the resulting dataframe.\n",
    "\n",
    "**The part of the result on the sample dataset:**\n",
    "\n",
    "`...\n",
    "195 946408\n",
    "215 860111\n",
    "235 897176\n",
    "300 857973\n",
    "321 915545\n",
    "...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group the dataframe by columns userId and trackId and use function “count” of DF API.\n",
    "userTrack = data.groupBy(col(\"userId\"), col(\"trackId\")).count();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each user take top-1000 and normalize them.\n",
    "#Sort the resulting Data Frame in descending order by the column norm_weight, \n",
    "#and then in ascending order this time first by “id1”, then by “id2”.\n",
    "userTrackNorm = (norm(userTrack, \"userId\", \"trackId\", \"count\", 1000) \\\n",
    "    .orderBy(col(\"norm_count\").desc(), col(\"userId\"), col(\"trackId\"))\\\n",
    "    .limit(40)).cache();    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.orderBy(col(\"norm_count\"));\n",
    "\n",
    "#Take top 40 rows, select only the columns “id1”, “id2”, and print the columns “id1”, “id2” of the resulting dataframe.\n",
    "userTrackList = userTrackNorm.withColumn(\"position\", rank().over(window))\\\n",
    "    .filter(col(\"position\") < 50)\\\n",
    "    .select(col(\"userId\").alias(\"id1\"), col(\"trackId\").alias(\"id2\"))\\\n",
    "    .take(40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 965774\n",
      "116 867268\n",
      "128 852564\n",
      "131 880170\n",
      "195 946408\n",
      "215 860111\n",
      "235 897176\n",
      "300 857973\n",
      "321 915545\n",
      "328 943482\n",
      "333 818202\n",
      "346 864911\n",
      "356 961308\n",
      "428 943572\n",
      "431 902497\n",
      "445 831381\n",
      "488 841340\n",
      "542 815388\n",
      "617 946395\n",
      "649 901672\n",
      "658 937522\n",
      "662 881433\n",
      "698 935934\n",
      "708 952432\n",
      "746 879259\n",
      "747 879259\n",
      "776 946408\n",
      "784 806468\n",
      "806 866581\n",
      "811 948017\n",
      "837 799685\n",
      "901 871513\n",
      "923 879322\n",
      "934 940714\n",
      "957 945183\n",
      "989 878364\n",
      "999 967768\n",
      "1006 962774\n",
      "1049 849484\n",
      "1057 920458\n"
     ]
    }
   ],
   "source": [
    "#for val in userTrackList:\n",
    "#    print (\"%s %s\" % val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph based Music Recommender. Task 3\n",
    "Build the edges of the type “user-artist”. Take the amount of times the user has listened to the artist’s tracks as the weight of the edge from the user’s vertex to the artist’s vertex.\n",
    "\n",
    "**Tip:** group the dataframe by the columns userId and trackId and use the function “count” of DF API. For each user take top-100 artists and normalize weights.\n",
    "\n",
    "Sort the resulting Data Frame in descending order by the column norm_weight, and then in ascending order this time first by “id1”, then by “id2”. Take top 40 rows, select only the columns “id1”, “id2”, and print the columns “id1”, “id2” of the resulting dataframe.\n",
    "\n",
    "**The part of the result on the sample dataset:**\n",
    "\n",
    "`...\n",
    "131 983068\n",
    "195 997265\n",
    "215 991696\n",
    "235 990642\n",
    "288 1000564\n",
    "...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "userArtist = data.groupBy(col(\"userId\"), col(\"artistId\")).count();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "userArtistNorm = (norm(userArtist, \"userId\", \"artistId\", \"count\", 100)\\\n",
    "                  .orderBy(col(\"norm_count\").desc(), col(\"userId\"), col(\"artistId\"))\\\n",
    "                  .limit(40)).cache();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.orderBy(col(\"norm_count\"));\n",
    "\n",
    "userArtistList = userArtistNorm.withColumn(\"position\", rank().over(window))\\\n",
    "    .filter(col(\"position\") < 40)\\\n",
    "    .select(col(\"userId\").alias(\"id1\"), col(\"artistId\").alias(\"id2\"))\\\n",
    "    .take(40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 993426\n",
      "116 974937\n",
      "128 1003021\n",
      "131 983068\n",
      "195 997265\n",
      "215 991696\n",
      "235 990642\n",
      "288 1000564\n",
      "300 1003362\n",
      "321 986172\n",
      "328 967986\n",
      "333 1000416\n",
      "346 982037\n",
      "356 974846\n",
      "374 1003167\n",
      "428 993161\n",
      "431 969340\n",
      "445 970387\n",
      "488 970525\n",
      "542 969751\n",
      "612 987351\n",
      "617 970240\n",
      "649 973851\n",
      "658 973232\n",
      "662 975279\n",
      "698 995788\n",
      "708 968848\n",
      "746 972032\n",
      "747 972032\n",
      "776 997265\n",
      "784 969853\n",
      "806 995126\n",
      "811 996436\n",
      "837 989262\n",
      "901 988199\n",
      "923 977066\n",
      "934 990860\n",
      "957 991171\n",
      "989 975339\n",
      "999 968823\n"
     ]
    }
   ],
   "source": [
    "for val in userArtistList:\n",
    "    print (\"%s %s\" % val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
